\chapter{Thermomechanical problem} \label{ch:thermomechanical_problem}

This chapter is adapted from \cite{vila-chaNumericalAssessmentPartitioned2023a} and provides a fundamental description of a solid material's response to mechanical and thermal loads in a finite strain setting.
It covers the use of thermodynamics with internal variables and the corresponding restrictions on the constitutive description of the material.
It also includes a discussion on the methods available in the literature for solving the thermomechanical problem.

\section{Kinematics of deformation} \label{sec:kinematics_of_deformation}

Let a deformable body $\mathscr{B}$\nomenclature[V]{$\mathscr B$}{Deformable body} occupy an open region $\Omega_0$ of the tridimensional Euclidean space $\mathscr{E}$ with a regular boundary $\partial \Omega_0$ in its reference configuration.
A smooth one-to-one function defines its motion \(\bm{\varphi}\colon \Omega\times \mathbb{R}\to \mathscr{E}\)\nomenclature[V]{$\bm \varphi$}{Motion}, mapping each material particle of coordinates $\bm{X}$\nomenclature[V]{$\bm{X}$}{Material point coordinates} in the reference configuration to its position $\bm{x}$\nomenclature[V]{$\bm x$}{Spatial point coordinates} in the deformed configuration.
Accordingly, the displacement is defined as \(\bm{u}\equiv \bm{x} - \bm{X}\)\nomenclature[V]{$\bm u$}{Displacement}.
In this work, the finite deformation of the body is described with respect to the initial configuration, following the so-called Lagrangian or material description.
The well-known deformation gradient second-order tensor is defined as \(\bm{F}(\bm{X},t)\equiv \nabla_0\bm\varphi(\bm{X},t)\)\nomenclature[V]{$\bm F$}{Deformation gradient}, and its determinant, denoted as \(J\equiv \text{det}\;\bm{F} \geq 0\)\nomenclature[V]{$J$}{Determinant of the deformation gradient} represents the local unit volume change.
Also relevant to the present work is the spatial velocity gradient $\bm L\equiv \dot{\bm F}{\bm F}^{-1}$, where $\dot{\bm F}$ denotes the time derivative of the deformation gradient and the dependence of the spatial coordinates and time is omitted.

\section{Fundamental conservation principles} \label{sec:fundamental_conservation_princ}

In continuum thermomechanics, there is a set of conservation principles and thermodynamic laws that, irrespective of the quantities used to describe the mechanical behavior of a body undergoing large deformations, must always be satisfied, namely,
\enlargethispage{\baselineskip}
\begin{align}
  \text{div}_0\;\bm{P} + \bm{b}_0 = \rho_0 \ddot{\bm{u}},\quad & \text{(balance of momentum)};
                                                                 \label{eq:material_equilibrium}\\
  \bm{F}^{-1}\bm{P} = \bm{P}^{T}\bm{F} ^{-T},\quad & \text{(balance of moment of momentum)};\\
  \rho_0 \dot e = \bm{P} :\dot{\bm{F}} + \rho_0 r -\operatorname{div}_0 \bm{q}_0,\quad & \text{(balance of energy)};\label{eq:balance_energy}\\
  \rho_0 \dot s + \operatorname{div}_0 \left[\frac{\bm{q}_0}{T}\right] - \frac{\rho_0 r}{T} \geq 0,\quad & \text{(entropy production inequality)}\label{eq:entropy_production},
\end{align}
where $\bm{b}_0$ is the body forces field \nomenclature[V]{$\bm{b}$}{Body forces}, measured in force per unit undeformed volume; $\rho_0$\nomenclature[V]{$\rho$}{Density} is the material density, measured in mass per unit undeformed volume; \(\bm{P}\)\nomenclature[V]{$\bm P$}{First Piola-Kirchhoff stress tensor} is the first Piola-Kirchhoff stress tensor; \(e\)\nomenclature[V]{$e$}{Specific internal energy} is the internal energy per unit mass; \(r\)\nomenclature[V]{$r$}{Specific heat supply} is the heat supply per unit mass; \(\bm{q}_0\)\nomenclature[V]{$\bm q$}{Heat flux vector} the first Piola-Kirchhoff heat flux vector, measured in heat power per unit undeformed surface; $T$\nomenclature[V]{$T$}{Absolute temperature} is the absolute temperature; and $s$\nomenclature[V]{$s$}{Specific entropy} is specific entropy per unit mass.
The second order tensor $\dot{\bm{F}}$ is the appropriate strain rate measure, such that the double contraction $\bm{P}:\dot{\bm{F}}$ represents the stress power per unit volume in the undeformed configuration of the body.

\section{Thermodynamically consistent constitutive modeling} \label{sec:constitutive_modeling}
The stresses and heat fluxes in the governing equations need to be associated with the deformations and temperatures via constitutive laws that represent the physical behavior of the material.
For a simple material, the thermodynamic state is assumed to be completely defined by the instantaneous values of a finite number of state variables, i.e., \(\{\bm{F}, T, \bm{g}_0, \bm{\alpha}\}\), where $\bm g_0 \equiv \nabla_0 T$\nomenclature[V]{$\bm g$}{Temperature gradient} is the material gradient of the temperature and \(\bm{\alpha} \equiv \{\alpha_k\}\)\nomenclature[V]{$\bm \alpha$}{Set of internal variables} is a set of internal variables, scalar or tensorial, associated with dissipative mechanisms.
This approach is included within the so-called rational thermodynamics as developed by Truesdell, Noll, Coleman and others \citep{nollMathematicalTheoryMechanical1958, colemanFoundationsLinearViscoelasticity1961, colemanExistenceCaloricEquations1964}.
It is a framework broad enough to describe thermodynamically irreversible processes, i.e., processes involving dissipation, thus allowing the modeling of physical phenomena relevant in terms of engineering practice.
A thorough review of the different approaches to the thermodynamics of irreversible processes can be found in \cite{lavenda1978thermodynamics}.

The constitutive description of the material must be consistent with the principles established by Equations~\eqref{eq:material_equilibrium}-\eqref{eq:entropy_production}, yielding the following constitutive relations
\begin{gather}
  \bm{P} = \rho_0 \frac{\partial \psi}{\partial \bm{F}},\label{eq:constitutive_equation_stress_thermoelasticity}\\
  s = - \frac{\partial \psi}{\partial T},\\
  \psi = \psi(\bm{F},T, \bm{\alpha}),\label{eq:thermo_mech_helmholtz_free_energy}\\
  \dot{\bm{\alpha}} = f(\bm{F}, T, \bm{g}_0,\bm{\alpha}),\\
  \bm{q}_0 = g(\bm{F}, T, \bm{g}_0, \bm{\alpha}),
\end{gather}
where \(\psi \equiv e - T s\)\nomenclature[V]{$\psi$}{Specific Helmholtz free energy} denotes the Helmholtz free energy.
These still need to comply with the second law of thermodynamics, which places constraints on the evolution equations for the internal variables and the constitutive equation for the heat flux.

The development of concrete models that are framed within such a constitutive theory can be achieved by postulating suitable functions for the  Helmholtz free energy and other ingredients, such as dissipation potentials and yield surfaces (see Chapter~\ref{ch:modeling_semi_crystalline_polymer} for more details).
Regarding the constitutive model for the heat flux, the second law of thermodynamics essentially requires the heat flow to occur in the opposite direction of the temperature gradient.
The Fourier heat conduction law for isotropic conduction in the deformed volume is one of the simplest and most popular alternatives, defining the heat flux as
\begin{equation}
  \bm{q}_0 = - k_0 \bm{C}^{-1} \bm{g}_0,
\end{equation}
where \(k_0\)\nomenclature[V]{$k$}{Thermal conductivity} is the thermal conductivity and \(\bm{C} = \bm{F}^T \bm{F}\)\nomenclature[V]{$\bm C$}{Right Cauchy-Green strain tensor} is the right Cauchy-Green strain tensor.


\section{Heat conduction equation}
\label{sec:heat-cond-equat}

In the context of thermomechanics, the most common form of the energy balance equation (Equation~\eqref{eq:balance_energy}) is the heat conduction equation.
Let \(C_{\bm F}\)\nomenclature[V]{$C_{\bm F}$}{Specific heat at constant deformation} denote the specific heat at constant deformation , i.e., the amount of heat required to change a unit mass of a substance by one degree in temperature at fixed deformation, defined as
\begin{equation} \label{eq:def_cv_partial}
  C_{\bm F}\equiv \left.\frac{\partial e}{\partial T}\right|_{\bm F}=-\frac{\partial^{2} \psi}{\partial T^{2}} T=\frac{\partial s}{\partial T} T.
\end{equation}
Taking Equation~\eqref{eq:def_cv_partial}, introducing the so-called Gough-Joule effect or thermoelastoplastic heating (or cooling) effect, \(\mathcal H^\text{ep}\)\nomenclature[V]{$\mathcal H^\text{ep}$}{Gough-Joule heating/cooling effect}, as
\begin{equation}
  \label{eq:def_gough_joule_effect}
  \mathcal H^\text{ep} = - \rho_0T\left(\frac{\partial^2 \psi}{\partial \bm{F}\partial T}: \dot{\bm{F}} + \frac{\partial^2 \psi}{\partial \bm{\alpha} \partial T}*\dot{\bm{\alpha}} \right),
\end{equation}
and the internal dissipation \(\mathcal D_\text{int}\)\nomenclature[V]{$\mathcal D_\text{int}$}{Internal dissipation}, given by
\begin{equation}
  \mathcal D_\text{int} = \bm{P}:\dot{\bm{F}} - \rho_0(\dot \psi + \dot T s),
\end{equation}
the energy balance equation can be recast as
\begin{equation}
  \label{eq:heat_conduction}
  \rho_0 C_{\bm F} \dot T = \rho_0 r - \operatorname{div}_0 \bm{q}_0 + \mathcal D_\text{int} + \mathcal H^\text{ep}.
\end{equation}

\section{Weak equilibrium and the principle of virtual work}

Together with the set of boundary and initial conditions, Equations~\eqref{eq:material_equilibrium} and \eqref{eq:balance_energy} are the so-called strong, point-wise, or local equilibrium equations, as they enforce the balance of momentum and energy at every material particle of the body.
The weak form of the linear momentum and energy balance equations can be formulated through the Virtual Work Principle as
\begin{equation} \label{eq:weak_momentum_balance}
  \int_{\Omega_0} [\bm{P}(\bm{F},T):\nabla_0 \bm{\eta} - (\bm{b}_0(t)-\rho_0\ddot{\bm{u}}(t))\cdot \bm{\eta}]d v - \int_{\partial\Omega_0} (\bm{P}\bm{n}_{0})\cdot \bm{\eta} \ \mathrm{d} a = 0\;,
\end{equation}
\begin{multline} \label{eq:weak_energy_balance}
  \int_{\Omega_0}   \left[-\bm{q}_{0}(\bm{F},T)\cdot \nabla_0 \xi - \left(\mathcal D_\text{int}(\bm{F},T)+\mathcal H^\text{ep}(\bm{F},T)+ \rho_0 r(t)-\rho_0 C_{\bm F}\dot T(t)\right) \xi\right]d v\\ + \int_{\partial\Omega_0} \bm{q}_{0}\cdot \bm{n}_{0} \xi \ \mathrm{d} a = 0,
\end{multline}
where, at each point of $\mathscr{B}$, the first Piola-Kirchhoff stress tensor is the solution to the material thermomechanical constitutive initial value problem and the virtual displacements \(\bm{\eta}\)\nomenclature[V]{$\bm \eta$}{Virtual displacements} and virtual temperatures \(\xi\)\nomenclature[V]{$\xi$}{Virtual temperatures} satisfy the essential boundary conditions of the problem in a homogeneous sense.
The coupling between the mechanical and thermal fields can be understood from a physical point of view as follows:
\begin{itemize}
\item the temperature influences the mechanical field through additional thermal stresses and potentially temperature-dependent material properties;
\item  the mechanical field affects the thermal field through coupling terms which can be interpreted as heat sources (dissipation and thermomechanical structure heating) and through geometric coupling due to the deformation of the domain, which affects boundary conditions and the heat conduction law.
\end{itemize}

\section{Finite Element Method} \label{sec:fem_mech}

It is now possible to apply the Finite Element Method to the solution of the thermomechanical initial boundary value problem, providing a suitable spatial discretization in a finite element mesh.
Defining the global vector of nodal displacements \(\mathbf{u}\)\nomenclature[V]{$\mathbf u$}{Global vector of nodal displacements} and the global vector of nodal temperatures \(\bm{\uptheta}\)\nomenclature[V]{$\bm\uptheta$}{Global vector of nodal temperatures} as
\begin{gather}
  \mathbf{u}(t) = \Big[ u_1^1(t),\dots,u^1_{n_\text{dim}}(t),\dots, u_1^{n_\text{nod}}(t),\dots,u^{n_\text{nod}}_{n_\text{dim}}(t)\Big]^T,\\
  \bm{\uptheta}(t) = \left[ T^1(t), T^2(t), \dots, T^{n_\text{nod}}(t)\right]^T,
\end{gather}
where \(n_\text{dim}\)\nomenclature[V]{$n_\text{dim}$}{Number of spatial dimensions} denotes the number of spatial dimensions and \(n_\text{nod}\)\nomenclature[V]{$n_\text{nod}$}{Number of nodes in the mesh} the total number of nodes in the mesh, the displacement, $\bm{u}(\bm{X}, t)$, and temperature, \(T(\bm{X}, t)\), fields defined over the global domain $\Omega_0$ can be approximated at any point $\bm{X}$ by appropriate interpolation functions.
This yields the discretized versions of the momentum and energy balance equations
\begin{gather}
  \mathbf{r}_{u}(\mathbf{u}, \bm{\uptheta}, t)\equiv\mathbf{M} \ddot{\mathbf{u}}(t) +\mathbf{f}_u^\text{\;int}(\bm{\uptheta}(t), \mathbf{u}(t))-\mathbf{f}^\text{\;ext}_{u}(\bm{\uptheta}(t), \mathbf{u}(t), t)=\mathbf{0}, \label{eq:disc_momentum_balance} \\
  \mathbf{r}_{T}(\mathbf{u}, \bm{\uptheta}, t)\equiv \mathbf{C} \dot{\bm{\uptheta}}(t)  + \mathbf{f}_T^\text{\;int}(\bm{\uptheta}(t), \mathbf{u}(t))-\mathbf{f}^\text{\;ext}_{T}(\bm{\uptheta}(t), \mathbf{u}(t), t)=\mathbf{0}, \label{eq:disc_energy_balance}
\end{gather}
which are written in residual form, with $\mathbf r_u$ the mechanical residual and $\mathbf r_T$ the thermal residual, where $\mathbf{f}_u^\text{\;int}$\nomenclature[V]{$\mathbf f^\text{int}$}{Internal nodal force} and $\mathbf{f}^\text{\;ext}_{u}$\nomenclature[V]{$\mathbf f^\text{ext}$}{External nodal force} are the mechanical global vectors of internal and external forces, $\mathbf{f}_T^\text{\;int}$ and $\mathbf{f}^\text{\;ext}_{T}$ are the thermal global vectors of internal and external forces, $\mathbf{M}$\nomenclature[V]{$\mathbf M$}{FEM mass matrix} is the mass matrix, and \(\mathbf{C}\)\nomenclature[V]{$\mathbf C$}{FEM thermal capacitance matrix} is the thermal capacitance matrix.
The previous matricial entities are usually obtained by the appropriate assemblage of their elemental counterparts, defined by the corresponding integral quantities.

\section{Time discretization}

In the context of thermomechanical problems, a general path-dependent constitutive model depends on both the instantaneous deformation and temperature states as well as their history.
Under these circumstances, for complex deformation, $\bm{F}(t)$, or temperature paths, $T(t)$, the solution of the constitutive initial value problem for a given set of initial conditions is typically unknown.
Therefore, a suitable numerical approach is necessary to integrate the rate constitutive equations, being an implicit backward-Euler scheme adopted in the present contribution.

The space-discrete time-continuous equilibrium equations (Equations~\eqref{eq:disc_momentum_balance} and \eqref{eq:disc_energy_balance}) can be integrated by employing an adequate and robust time discretization scheme.
The fully discrete problem can be written as \nomenclature[V]{$\mathbf r$}{Nodal residue}
\begin{gather}
  \mathbf{r}_{u}^{n+1}(\mathbf{u}_{n+1}, \bm{\uptheta}_{n+1}, t_{n+1})=\bm{0}\,\label{eq:mech_problem}\\
  \mathbf{r}_{T}^{n+1}(\mathbf{u}_{n+1}, \bm{\uptheta}_{n+1}, t_{n+1}) = \bm{0}. \label{eq:therm_problem}
\end{gather}
For quasi-static structural problems and steady-state heat flow problems, the temporal integration is fully restrained at the constitutive level, as previously addressed.
For transient problems, the Generalised-$\alpha$ method is a popular alternative, which establishes finite-difference approximations of the temporal derivatives and evaluates the equilibrium at generalised midpoints, providing enough freedom to have second-order accuracy, unconditional stability in linear problems and optimal numerical dissipation in terms of a sole parameter $\rho_{\infty}$.
Unless otherwise stated, the Generalised-$\alpha$ for first-order systems is employed to integrate the transient thermal response \citep{jansen2000GeneralizedaMethodIntegrating}.

\section{Solution of the thermomechanical problem}

In the present work, implicit partitioned schemes are adopted to solve the thermomechanical problem.
The cornerstone of partitioned solution schemes is to solve the thermal and mechanical problems separately, i.e., Equation~\eqref{eq:mech_problem} is solved considering a fixed temperature, and Equation~\eqref{eq:therm_problem} is solved assuming a fixed configuration.
For convenience, consider the existence of two functions, \(\bm{\mathcal{U}}_{n+1}\)\nomenclature[V]{$\bm{\mathcal{U}}$}{Solution procedure for the mechanical problem at constant temperature} and \(\bm{\mathcal{T}}_{n+1}\)\nomenclature[V]{$\bm{\mathcal{T}}$}{Solution procedure for the thermal problem at a fixed configuration}, that represent these solution procedures at instant \(t_{n+1}\), such that
\begin{align}
  \mathbf{u} = \bm{\mathcal{U}}_{n+1}(\bm{\uptheta}) & \rightarrow \text{solve } \mathbf{r}_{u}^{n+1}(\mathbf{u}, \bm{\uptheta}, t_{n+1})=\bm{0} \text{ in order to obtain } \mathbf{u}\;, \label{eq:def_solvers_u} \\
  \bm{\uptheta} = \bm{\mathcal{T}}_{n+1}(\mathbf{u}) & \rightarrow \text{solve } \mathbf{r}_{T}^{n+1}(\mathbf{u}, \bm{\uptheta}, t_{n+1})=\bm{0} \text{ in order to obtain } \bm{\uptheta}. \label{eq:def_solvers_t}
\end{align}
In the following, the time-step subscripts $(\bullet)_{n+1}$ on the solvers are dropped for notation compactness.

The standard conceptual approach found in the literature for implicit solution schemes is to adopt a fixed-point scheme, such as
\begin{equation}
  \bm{\uptheta}^{k}_* = \bm{\mathcal{T}} \circ \bm{\mathcal{U}}(\bm{\uptheta}^k) \quad \text{or} \quad \mathbf{u}^{k}_* = \bm{\mathcal{U}} \circ \bm{\mathcal{T}}(\mathbf{u}^k),
\end{equation}
where \(\circ\) denotes function composition.
The solution found from the fixed-point scheme, \(\bm{\uptheta}^{k}_*\) or \(\mathbf{u}^{k}_*\) can then be accelerated, i.e.,
\begin{equation}
  \bm{\uptheta}^{k+1} = \bm{\mathcal{A}}(\bm{\uptheta}^{k}_*) \quad \text{or} \quad \mathbf{u}^{k+1} = \bm{\mathcal{A}}(\mathbf{u}^{k}_*),
\end{equation}
with \(\pazocal{A}\) denoting an appropriate acceleration scheme, which may also use previous iterations.
The superscript \(k\) denotes the nonlinear iterations performed within each time step.

A slightly different conceptualization of the thermomechanical coupled problem is pursued here.
In general, a fixed-point procedure can be transformed into a root-finding problem.
In this case, the goal is to define suitable functions, built from \(\bm{\mathcal{U}}\) and \(\bm{\mathcal{T}}\), whose roots are also the solutions to the thermomechanical problem (Equations~\eqref{eq:mech_problem} and \eqref{eq:therm_problem}).
In the thermomechanical context, the following residual functions \nomenclature[V]{$\bm{\mathcal R}$}{Thermomechanical residual} are employed
\begin{equation} \label{eq:def_res_jacobi}
  \bm{\mathcal{R}}_\text{J}(\mathbf{u}, \bm{\uptheta}) =
  \left\{\begin{array}{c}
           \mathbf{u} - \bm{\mathcal{U}}(\bm{\uptheta})\\
           \bm{\uptheta} - \bm{\mathcal{T}}(\mathbf{u})
         \end{array}\right\},
     \end{equation}
     and
     \begin{equation} \label{eq:def_res_gauss_seidel}
       \bm{\mathcal{R}}_\text{GS}(\bm{\uptheta}) =
       \bm{\uptheta} - \bm{\mathcal{T}}\circ \bm{\mathcal{U}}(\bm{\uptheta}) \quad \text{or} \quad \bm{\mathcal{R}}^*_\text{GS}(\mathbf{u}) =
       \mathbf{u} - \bm{\mathcal{U}}\circ \bm{\mathcal{T}}(\mathbf{u}),
     \end{equation}
     where the subscript 'J' stands for Jacobi and the subscript 'GS' for Gauss-Seidel.
     It should be noted that the residuals, as given here, are the symmetric counterparts of the definitions commonly employed in FSI.
    The residual \(\bm{\mathcal{R}}_\text{GS}\) (Equation~\eqref{eq:def_res_gauss_seidel}) coinciding with the isothermic split, where the mechanical problem is solved first at a fixed temperature, followed by the solution of the thermal problem at a fixed configuration is employed in the numerical results provided in the present work.
    Keep in mind that which of the fields is solved first may be critical for the approach's stability and convergence rate.  \citep{joosten_analysis_2009}.

     Since the methods described below for the solution of nonlinear systems of equations apply to both functions \(\bm{\mathcal{R}}_\mathrm{J}\) and \(\bm{\mathcal{R}}_\mathrm{GS}\), a general function denoted as \(\bm{\mathcal{R}}\), whose variable is \(\mathbf{x}\), is conveniently adopted in the following discussion.

     As previously stated, the solution to the thermomechanical problem (Equations~\eqref{eq:mech_problem} and \eqref{eq:therm_problem}) can be conceptually posed as the solution of
     \begin{equation} \label{eq:abstract_residue_equation}
       \bm{\mathcal{R}}(\mathbf{x}) = 0,
     \end{equation}
     where $\mathbf x$\nomenclature[V]{$\mathbf x$}{Abstract nodal quantity} stands for the appropriate unknowns based on the particular residual ($\bm \uptheta$ or $\mathbf u$, see Equation~\eqref{eq:def_res_gauss_seidel}).
     It should be mentioned that unknowns in all mesh nodes must be considered for a volumetric coupling, such as in a thermomechanical problem.
     This is in contrast with fluid-structure interaction, where just the degrees of freedom at the interface must be taken into account.
     For completeness, also consider the function
     \begin{equation}
       \bm{\mathcal{S}}(\mathbf{x}) = \mathbf{x} - \bm{\mathcal{R}}(\mathbf{x}),
     \end{equation}
     whose fixed-point is the solution to the nonlinear equation system in Equation~\eqref{eq:abstract_residue_equation}.
     Therefore, a broad class of standard implicit methods available in the literature can be applied to solve the problem at hand, allowing the use of appropriate libraries when available.
     The accelerated fixed-point counterparts can also be properly identified, as shown in the remainder of this section.

     In the present work, the criteria used for the choice of the most suitable implicit methods are similar to the ones provided by Fang and Saad \citep{fang_two_2009} in the context of electronic structure problems.
     These can be summarized as follows:
     \begin{enumerate}
     \item The dimensionality of the problem is large;
     \item \(\bm{\mathcal{R}}\) is continuously differentiable, but the analytical form of its derivative is not readily available or is computationally expensive to compute;
     \item The evaluation of \(\bm{\mathcal{R}}(\mathbf{x})\) is computationally demanding;
     \item The problem is noisy, i.e., the computed function values of \(\bm{\mathcal{R}}\) usually contain errors.
     \end{enumerate}
     Attending to the previous criteria, the most suitable methods should comply with the following desirable features: it must minimize the number of calls to \(\bm{\mathcal{R}}\), as it is expensive to compute; the amount of information saved from previous iterations must be judiciously chosen as the problem's dimensionality is large; and it cannot require the analytical derivative of \(\bm{\mathcal{R}}\), since it is not available.

     % In general, any implicit method for solving nonlinear systems of equations available in the literature can be used to solve the partitioned thermomechanical problem as long as it meets these criteria.
     % The approaches considered by the author in \cite{vila-chaNumericalAssessmentPartitioned2023a}: the fixed-point method, the constant underrelaxation method, the Aitken relaxation method, the Broyden-like methods, especially Broyden's method, the Newton-Krylov methods, and the polynomial vector extrapolation methods in cycling mode.

     % \smallskip
     % \noindent \textit{Remark.} To make it clear to the reader, each time $\bm{\mathcal{R}}(\bullet)$ appears in the formulas; it represents a new execution to the solution sequence of the fields, which requires new calls to the mechanical solver, thermal solver and data communication.
     % \smallskip

     \smallskip
          \noindent \textit{Remark.} To clarify, each appearance of $\bm{\mathcal{R}}(\bullet)$ in the formulas entails new calls to the mechanical solver, thermal solver, and data communication according to its particular definition.
          \smallskip

          \subsection{Fixed-point} \label{sec:fixed_point_approach}

          The application of the fixed-point method to obtain the roots of \(\bm{\mathcal{R}}\) yields
          \begin{equation}
            \mathbf{x}^{k+1} = \bm{\mathcal{S}}(\mathbf{x}^k) = \mathbf{x}^k - \bm{\mathcal{R}}(\mathbf{x}^k).
          \end{equation}
          If the particular functions defined in Equations~\eqref{eq:def_res_jacobi} and \eqref{eq:def_res_gauss_seidel} are used, one finds the  two basic Schwarz procedures commonly employed in partitioned implicit solution procedures \citep{uekermann_parallel_2013, danowski_computational_2014, gatzhammer_efficient_2014}.
          They are the additive or block Jacobi and the parallel Scharwz or Gauss-Seidel procedures.
          The names originate from domain decomposition, and justify the subscripts employed in Equations~\eqref{eq:def_res_jacobi} and \eqref{eq:def_res_gauss_seidel}.

          This approach necessitates only one residual evaluation per nonlinear iteration, with no previous iterations being required.
          Its computational complexity scales linearly with the number of unknowns, and it is the simplest method to implement.

          \subsection{Constant underrelaxation} \label{sec:underrelaxation}

          One of the most straightforward ways to stabilize an iterative method is to use constant underrelaxation (see, e.g., \cite{erbts_accelerated_2012} or \cite{gatzhammer_efficient_2014}).
          The relaxation is performed as follows
          \begin{equation} \label{eq:constant_relaxation}
            \mathbf{x}^{k+1}=(1-\omega) \mathbf{x}^{k}+\omega\bm{\mathcal{S}}(\mathbf{x}^k)=\mathbf{x}^{k} -\omega \bm{\mathcal{R}}(\mathbf{x}^k),
          \end{equation}
          where \(\omega\) is the relaxation factor chosen in the range \(0<\omega<1\), which corresponds to an underrelaxation, to achieve a stabilizing effect.

          Constant underrelaxation works well if \(\omega\) is close to 1 but leads to a slow convergence if \(\omega\) has to be chosen close to 0.
          Thus, the constant underrelaxation method creates unmanageable computational costs for severe instabilities.
          Overrelaxation can also be considered, keeping in mind that for \(\omega > 2\), convergence is lost.
          The optimal \(\omega\) is not necessarily the largest stable one \citep{gatzhammer_efficient_2014} and has to be set empirically.
          In what follows, alternative methods are discussed to decrease the number of iterations necessary while maintaining stability.

          The number of residual evaluations, memory requirements, and computation complexity equals the ones displayed by the fixed-point approach.
          Compared to the last approach, the increase in complexity is negligible.

          \subsection{Aitken relaxation} \label{sec:aitken_relaxation}


          The so-called Aitken \(\Delta^2\) relaxation method was introduced by Irons and Tuck \citep{irons_version_1969} as a modified Aitken \(\Delta^2\) that does not require the computation of the function twice per iteration as in the original method \footnote{The use of the Aitken-\(\Delta^2\) process to solve nonlinear equations leads to Steffenson's method. There are also generalizations of the Aitken-\(\Delta^2\) process to the vector case \citep{sidi_vector_2017}. Both lead, however, to schemes where the residual is evaluated twice per nonlinear iteration.}.
          It has been widely used in the context of FSI \citep{irons_version_1969, kuttler_fixed-point_2008, joosten_analysis_2009, kuttler_vector_2009} and also in thermomechanics (see, e.g.,\cite{erbts_accelerated_2012, danowski_computational_2014, erbts_partitioned_2015, wendt_partitioned_2015}).

          In the one-dimensional case, this method resembles the secant method applied to the fixed point problem, which can be used to solve nonlinear equations without differentiation.
          This version of Aitken's \(\Delta^2\) method provides a dynamic relaxation parameter, which can be used to improve the convergence/stability properties of the coupling algorithm.

          The solution to the current iteration from the outcome of the previous iteration $\mathbf{x}^{k}$ plus a new increment $\Delta \mathbf{x}^{k}$ is found from
          \begin{equation}
            \mathbf{x}^{k+1}=\mathbf{x}^{k}+\Delta \mathbf{x}^{k}.
          \end{equation}
          The increment reads
          \begin{equation} \label{eq:aitken_update}
            \Delta \mathbf{x}^{k}=\omega^{k}\left(\bm{\mathcal{S}}(\mathbf{x}^{(k)})-\mathbf{x}^{(k)}\right)=-\omega^{k} \bm{\mathcal{R}}(\mathbf{x}^k),
          \end{equation}
          with $\omega^{k}$ being the relaxation coefficient.
          This coefficient is updated at every iteration cycle as a function of two previous residuals,
          \begin{equation} \label{eq:aitken_relaxation_factor}
            \omega^{k}=-\omega^{k-1} \frac{\left(\bm{\mathcal{R}}(\mathbf{x}^k)-\bm{\mathcal{R}}(\mathbf{x}^{k-1})\right)^{\mathrm{T}} \bm{\mathcal{R}}(\mathbf{x}^{k-1})}{\left(\bm{\mathcal{R}}(\mathbf{x}^k)-\bm{\mathcal{R}}(\mathbf{x}^{k-1})\right)^{2}}.
          \end{equation}
          The dynamical relaxation coefficient is restricted to the range \((0,2)\) because employing a relaxation coefficient outside this range leads to a loss of convergence \citep{erbts_accelerated_2012}.
          For the first nonlinear iteration, an initial value for the relaxation coefficient must be specified.
          If $\omega^{0}=1$ is chosen, the result will be a fixed-point iteration, then followed by dynamic relaxation.

          This approach is also easy to implement, and the additional computational input is acceptable since only inner vector products must be performed.
          It only requires a residual evaluation per iteration and the residual value of the previous iteration.

          \subsection{Newton-Krylov methods} \label{sec:newton_krylov}

          The Newton-Raphson or Newton scheme is a very popular iterative solution procedure for nonlinear systems of equations.
          When certain mathematical conditions are met, this scheme converges quadratically.
          It can be applied to Equation~\eqref{eq:abstract_residue_equation}, yielding
          \begin{gather}
            \bm{\mathbf{J}}_{\bm{\mathcal{R}}(\mathbf{x}^k)}\Delta \mathbf{x}^k = - \bm{\mathcal{R}}(\mathbf{x}^k), \label{eq:newton_system}\\
            \mathbf{x}^{k+1} = \mathbf{x}^k + \Delta \mathbf{x}^k. \label{eq:newton_iter}
          \end{gather}
          Every iteration of the Newton scheme involves at least one invocation of the thermal and mechanical solvers when computing $\bm{\mathcal{R}}\left(\mathbf{x}^{k}\right)$.
          The critical point for black-box equation coupling is obtaining the derivative information in the Jacobian matrix, which is not readily available.

          One possibility to bypass the need for $\bm{\mathbf{J}}_{\bm{\mathcal{R}}}$ is by employing Newton-Krylov methods, which seek the solution of the Newton system of equations in Equation~\eqref{eq:newton_system} using Krylov methods, such as the generalized minimal residual method (GMRES) or the biconjugate gradient stabilized method (BiCGSTAB).
          The Krylov iterative methods approximate the solution of a linear system \(\mathbf{A} \mathbf{z} = \mathbf{b}\) using the Krylov subspace
          \begin{equation}
            \pazocal{K}_m = \operatorname{span}\{\mathbf{r}_0, \mathbf{A}\mathbf{r}_0, \mathbf{A}^2 \mathbf{r}_0, \dots, \mathbf{A}^{m-1}\mathbf{r}_0\},
          \end{equation}
          such that the \(m\)-th iterate  \(\mathbf{z}_m\in \pazocal{K}_m\), with \(\mathbf{r}_0 = \mathbf{b} - \mathbf{A} \mathbf{z}_0\).
          The precise way in which \(\mathbf{z}_m\) is built is what distinguishes the different methods.
          To generate the appropriate Krylov subspace, one needs the product \(\bm{\mathbf{J}}_{\bm{\mathcal{R}}}(\mathbf{x}^k) \mathbf{y}\) in Equation~\eqref{eq:newton_system}, for some vector \(\mathbf{y}\).
          It is assumed that the Jacobian is unavailable, so it must be approximated.
          Also, it would be beneficial if the full Jacobian is neither computed in its entirety nor fully stored in memory, i.e., a matrix-free method is desirable.
          Although the matrix of coefficients $\mathbf{A}=\bm{\mathbf{J}}_{\bm{\mathcal{R}}}$ is unknown, it is possible to exploit the fact that $\mathbf{A}$ is a Jacobian matrix, such that the Jacobian-vector product can be approximated with a numerical forward difference directional derivative \citep{kelley_solving_2003}.

          Since the present use case includes many unknowns, it leads to memory concerns if the Krylov subspace is allowed to grow indefinitely.
          A restarted version where the maximum size of the Krylov space is restricted to \(p\) elements is preferred.
          Once this number is reached, the procedure is restarted.
          However, the convergence can be poor if \(p\) is small.
          The so-called restarted GMRES is denoted as \(\text{GMRES}(p)\), with \(p\) being the maximum number of previous iterations in memory.
          Further details on Newton-Krylov methods can be found in \cite{kelley_solving_2003}.

          In each nonlinear iteration of the Newton-Krylov method, the number of iterations inside the Krylov subspace solver can be large, and each iteration requires an evaluation of the residual.
          This number of residual evaluations can be a significant drawback when the evalution of \(\bm{\mathcal{R}}\) is expensive.
          This problem is, however, mitigated by the fact that the linear system in Newton's method is only solved until it satisfies
          \begin{equation}
            \|\bm{\mathbf{J}}_{\bm{\mathcal{R}}}(\mathbf{x}^k) \Delta \mathbf{x}^*_m+ \bm{\mathcal{R}}(\mathbf{x}^k)\| \leq \eta\|\bm{\mathcal{R}}(\mathbf{x}^k)\|,
          \end{equation}
          where \(\eta\) is called the forcing term, and it is chosen to avoid over-solving the Newton system (Equation~\eqref{eq:newton_system})--- the linear system is solved to an error that does not influence the error in the nonlinear solution procedure.
          As a simple approach, Kelley \citep{kelley_solving_2003} suggests using a fixed value for the forcing term, e.g., \(\eta=0.1\).
          However, according to the same author there are more sophisticated ways to choose this parameter, such as the Eisenstat-Walker method.
          The smaller the forcing term \(\eta\), the closer one gets to the standard Newton method.
          However, especially in the first nonlinear iterations, choosing a \(\eta\) that is too small leads to unnecessarily long computational times since the error in the Newton iterations supersedes the error in the solution of the linear system.

          \subsection{Broyden-like class} \label{sec:multisecant}

          Both the generalized Broyden's family, including Broyden's method, and Anderson's mixing can be understood as methods in the Broyden-like class as described in \cite{fang_two_2009}.
          They are multisecant quasi-Newton methods which rely on the Newton update and build approximations to the Jacobian or to its inverse.

          Suppose the latest \(m\) iterates of the nonlinear solution sequence is available, which are denoted by \(\mathbf{x}^{1}, \ldots, \mathbf{x}^{m}\).
          Let \(\Delta \mathbf{x}^{i}=\mathbf{x}^{i+1}- \mathbf{x}^{i}\) for \(i=1, \ldots, m-1\).
          The $m-1$ step vectors, \(\Delta \mathbf{x}^{1}, \ldots, \Delta \mathbf{x}^{m-1}\), are partitioned into \(p\) groups,
          \begin{align}
            \mathbf{X}^{1} & =\left[\Delta \mathbf{x}^{1}, \ldots, \Delta \mathbf{x}^{z_{1}}\right], \\
            \mathbf{X}^{2} & =\left[\Delta \mathbf{x}^{z_{1}+1}, \ldots, \Delta \mathbf{x}^{z_{2}}\right],\\
                           & \vdots \\
            \mathbf{X}^{p} & =\left[\Delta \mathbf{x}^{z_{p-1}+1}, \ldots, \Delta \mathbf{x}^{z_{p}}\right],
          \end{align}
          where \(z_{i}\) is the index of the last entry in the \(i\)-th group for \(i=1, \ldots, p\), meaning that \(z_{0}=0\) and \(z_{p}=m-1\).
          Also partition \(\Delta \bm{\mathcal{R}}^{1}, \ldots, \Delta \bm{\mathcal{R}}^{m-1}\) into \(\mathbf R^{1}, \ldots, \mathbf R^p\) accordingly, where \(\Delta \bm{\mathcal{R}}^{i}=\bm{\mathcal{R}}^{i+1}- \bm{\mathcal{R}}^{i}\) with \(\bm{\mathcal{R}}^{i}=\bm{\mathcal{R}}(\mathbf{x}^{i})\).
          The sizes of the groups for \(i=1, \ldots, p\) are denoted by \(s_{i}\equiv z_{i}-z_{i-1}\).
          For a generic iteration $k\in[z_{i-1}+1, z_{i}]$ inside group $i$, the inverse of the Jacobian, \(\mathbf{G}_{\bm{\mathcal{R}}}\equiv \bm{\mathbf{J}}_{\bm{\mathcal{R}}}^{-1}\), is approximated as
          \begin{equation} \label{sec:broyden_like_class_g_update}
            \mathbf{G}_{\bm{\mathcal{R}}}^{k} = \mathbf{G}_{\bm{\mathcal{R}}}^{z_{i-1}}+\left(\mathbf{X}^{i-1}-\mathbf{G}_{\bm{\mathcal{R}}}^{z_{i-1}} \mathbf R^{i-1}\right) {\mathbf{V}^{i-1}}^T,
          \end{equation}
          where \({\mathbf{V}^{j}}^T \mathbf{R}^{j}=\mathbf{I}\) for the multi-secant condition
          \begin{equation}
            \mathbf{G}_{\bm{\mathcal{R}}}^{z_j+1} \mathbf R^j = \mathbf X^j,
          \end{equation}
          to be satisfied.
          The two optimal choices of \({\mathbf{V}^j}^T = {\mathbf{M}^j}^{-1}{\mathbf{N}^j}^T\) are
          \begin{align}
            \text{Type I: } \qquad & {\mathbf{M}^j} = {\mathbf X^j}^T \mathbf{G}_{\bm{\mathcal{R}}}^j \mathbf R^j, && \hspace{-2cm} {\mathbf{N}^j}^T = {\mathbf X^j}^T \mathbf{G}_{\bm{\mathcal{R}}}^j\;, \label{eq:broyden_type_i}\\
            \text{Type II: } \qquad & {\mathbf{M}^j} = {\mathbf R^j}^T \mathbf R^j, && \hspace{-2cm} {\mathbf{N}^j}^T = {\mathbf R^j}^T\;. \label{eq:broyden_type_ii}
          \end{align}
          The next iterate is set as
          \begin{equation} \label{eq:update_broyden_family}
            \mathbf{x}^{k+1}=\mathbf{x}^{k}-\mathbf{G}_{\bm{\mathcal{R}}}^{k} \bm{\mathcal{R}}^k.
          \end{equation}
          One still needs the initial approximation to the inverse of the Jacobian, \(\mathbf{G}_{\bm{\mathcal{R}}}^1\), whose size is \(N\times N\).
          In \cite{fang_two_2009}, the approach adopted was to follow the idea of Anderson's mixing and set
          \begin{equation}
            \mathbf{G}_{\bm{\mathcal{R}}}^0 = - \beta \mathbf{I}, \label{eq:initG}
          \end{equation}
          drastically improving the memory requirements, as only one scalar parameter, \(\beta\), needs to be saved.
          Also, Kelley \citep{kelley_solving_2003} assumes, in his implementation of Broyden's method, an initial approximation to \(\mathbf{G}_{\bm{\mathcal{R}}}^0\) equal to the identity matrix.
          Information about \(\mathbf{G}_{\bm{\mathcal{R}}}^0\) is applied in the preconditioning of the system instead.

          To clarify the procedure behind the previous expressions, the whole process departs from the initial estimate of the inverse of the Jacobian, given by Equation~\eqref{eq:initG}.
          The Newton-update (Equation~\eqref{eq:update_broyden_family}) is repeated $s_{1}=z_{1}-0$ times with a constant Jacobian (\(\mathbf{G}_{\bm{\mathcal{R}}}^0\)).
          At the beginning of iteration $k=z_{1}+1$, when the first group ($i=1$) is finalised and the second group ($i=2$) starts, the inverse of the Jacobian is updated with the $s_{i}$ previous steps using Equation~\eqref{sec:broyden_like_class_g_update}.
          Then, the Newton iterations are performed $s_{2}=z_{2}-z_{1}$ times with constant Jacobian \(\mathbf{G}_{\bm{\mathcal{R}}}^{z_{1}+1}\) and the process repeats over time until convergence is achieved.
          It is also common for the sizes of the \(p\) groups to be equal, i.e., \(s_i=s\), for \(i=1,\dots,m\).

          \paragraph{Broyden's method} Broyden's method \citep{broydenClassMethodsSolving1965a} with the identity matrix as the initial guess for the inverse of the Jacobian corresponds to using a group size equal to one, \(s=1\), \(\beta=-1\), and enforcing the secant condition according to Equation~\eqref{eq:broyden_type_i}, referring only to the previous two iterations.

          Regarding memory availability, the iteration can be restarted if there is no more room to store the vectors \citep{kelleySolvingNonlinearEquations2003}.
          A different approach, called limited memory in the optimization literature, replaces the oldest stored iterations with the most recent ones.

          A set of practical considerations must be taken into account and can be found in \cite{fang_two_2009}.
          These lead to a storage need of
          \begin{enumerate}
          \item  Two column vectors of size \(N\) for \(\mathbf{x}^{m}\) and \(\bm{\mathcal{R}}^{m}\).
          \item An \(N \times(m-1)\) matrix for \(\mathbf{X}^{1}, \ldots, \mathbf{X}^{k}\).
          \item An \(N \times(m-1)\) matrix for \(\mathbf{R}^{1}, \ldots, \mathbf{R}^{k}\) .
          \item For Type-I update, the last group \(\mathbf N^{k}\) is also stored since its computation involves \(\mathbf{G}_{\bm{\mathcal{R}}}^{k}\).
          \end{enumerate}
          and for each update of the inverse of the Jacobian, the computational cost is \(\pazocal{O}(Nm^2)\), due to the \(QR\) decomposition of an \(N\times m\) matrix needed in the computation of \({\mathbf V^j}^T\) (see Equations~\eqref{eq:broyden_type_i} and \eqref{eq:broyden_type_ii}).
          Kelley \citep{kelley_solving_2003} presents an implementation for Broyden's method that halves the memory requirement relative to the one presented in \cite{fang_two_2009}.

          Fang and Saad \citep{fang_two_2009} also employ a simple restarting procedure.\
          If in two consecutive values of \(\bm{\mathcal{R}}\), \(\bm{\mathcal{R}}_\text{old}\) and \(\bm{\mathcal{R}}_\text{new}\), \(\|\bm{\mathcal{R}}_\text{new}\|\) is much larger than \(\|\bm{\mathcal{R}}_\text{old}\|\), the solution procedure is restarted, with the new initial trial values corresponding to \(\bm{\mathcal{R}}_\text{old}\).
          They suggest \(r\) between 0.1 and 0.3, with \(\|\bm{\mathcal{R}}_\text{old}\| < r\|\bm{\mathcal{R}}_\text{new}\|\) leading to a restart.
          This approach is not used here as there is evidence that these methods perform well without it in the thermomechanical problems studied.


          \paragraph{In the context of FSI}

          The multi-secant quasi-Newton methods have been used in the context of FSI, e.g., the interface block quasi-Newton method with least-squares approximation (IBQN-LS) \citep{vierendeels_implicit_2007} and the Interface Quasi-Newton technique with an approximation for the Inverse of the Jacobian from a Least-Squares model (IQN-ILS) \citep{degroote2009performance}, which fit into the framework presented above.
          For more detailed treatments see, e.g., \cite{haelterman_quasi-newton_2009, gatzhammer_efficient_2014, uekermann_partitioned_2016, scheufele_coupling_2018}.

          \subsection{Vector extrapolation techniques in cycling mode} \label{sec:vector_extrapolation}

          There is vast literature on sequence acceleration/extrapolation methods (see  \cite{brezinski_extrapolation_2013} and \cite{sidi_vector_2017} for textbook treatments of this topic).
          Perhaps the most well-known acceleration method is the Aitken \(\Delta^2\) process, which has been mentioned in Section~\ref{sec:aitken_relaxation}, due to its popularity and inherent simplicity.
          They are based on the very natural idea of extrapolation.
          According to \cite{brezinski_extrapolation_2013}, there is a strong connection between sequence transformations and fixed point methods for solving \(x= g( x)\), \(g\colon \mathbb R\to \mathbb R\).
          The most well-known example of this connection is that between Aitken's \(\Delta^{2}\) process and Steffensen's method.

          Turning to vector sequences and systems of nonlinear equations, let \(F\) be a vector extrapolation method which takes the last $q+2$ vectors of $(\mathbf{w}^{0},...,\mathbf{w}^{p+q+1})$ computed from the sequence $\{\mathbf{w}\}^{i}$ and computes the extrapolated value as
          \begin{equation}
            \mathbf{s}_{p,q} = F\left(\mathbf{w}^{p}, \ldots, \mathbf{w}^{p+q+1}\right),
          \end{equation}
          with \(p\geq 0\) and \(q\geq 1\).
          For solving the fixed point problem \(\mathbf{x}=\bm{\mathcal{S}}(\mathbf{x})\), one can associate the iterative method \(F\) such that the $k$-th iteration of the nonlinear solution sequence is given by
          \begin{equation}
            \mathbf{x}^{k+1}=F\left(\bm{\mathcal{S}}^{p}(\mathbf{x}^{k}), \ldots, \bm{\mathcal{S}}^{p+q+1}(\mathbf{x}^{k})\right),
          \end{equation}
          where \(\bm{\mathcal{S}}^{i+1}(\mathbf{x})=\bm{\mathcal{S}} \circ \bm{\mathcal{S}}^{i}(\mathbf{x})\) and \(\bm{\mathcal{S}}^{0}(\mathbf{x})=\mathbf{x}\).
          This approach is called full cycling or simply cycling.
          Essentially, for each nonlinear iteration the fixed-point is applied  $p$ times in a recursive manner, and the last $q+1$ of this sequence is used to perform the extrapolation.

          Vector extrapolation methods are classified into two types: polynomial methods and methods based on the \(\epsilon\)-algorithm \citep{brezinski_extrapolation_2013, sidi_vector_2017}.
          In what follows, only the first category is considered since the second requires a relatively large number of function evaluations per iteration.
          Sidi \citep{sidi_vector_2017} presents four different polynomial extrapolation methods, that attempt to express the limit of the vector sequence as a linear combination of \(q+1\) iterates,
          \begin{equation}
            \mathbf{x}^{k+1} = \sum_{j=0}^{q} \gamma_{j} \bm{\mathcal{S}}^{p+j}(\mathbf{x}^{k})  = \sum_{j=0}^{q} \gamma_{j} \mathbf{w}^{p+j},
          \end{equation}
          where the notation shorthand $\mathbf{w}^{i} = \bm{\mathcal{S}}^{i}(\mathbf{x}^{k})$ has been introduced.
          It should be remarked that although the linear combination is performed over $q+1$ vectors, the previously mentioned $q+2$ vectors are still used in the computation of the coefficients $\gamma_{j}$.
          The methods considered in the present work are the minimal polynomial extrapolation (MPE) and the reduced rank extrapolation (RRE).

          \paragraph{MPE}

          Solve the overdetermined linear system \(\mathscr W^{q-1} \mathbf{c}^{\prime}=-\Delta \mathbf{w}^{p+q}\) in the least-squares sense for \(\mathbf{c}^{\prime}=\left[c_{0}, c_{1}, \ldots, c_{q-1}\right]^{T}\), where \(\mathbf{W}^{q-1} = [\Delta \mathbf{w}^{p}, \dots, \Delta \mathbf{w}^{p+q-1}]\) and \(\Delta \mathbf{w}^i = \mathbf{w}^{i+1} - \mathbf{w}^i\).
          With \(c_{0}, c_{1}, \ldots, c_{q-1}\) available, set \(c_{q}=1\) and compute \(\gamma_{j}=c_{j} / \sum_{i=0}^{q} c_{i}, j=0,1, \ldots, q\), provided \(\sum_{j=0}^{q} c_{j} \neq 0\).

          \paragraph{RRE}

          Solve the overdetermined linear system \(\mathbf{W}^{q} \boldsymbol\gamma=0\) in the least-squares sense, subject to the constraint \(\sum_{j=0}^{q} \gamma_{j}=1\), where \(\boldsymbol\gamma = [\gamma_0, \dots, \gamma_q]\).

          \bigskip

          For a combination \((p,q)\), the required number of residual evaluations per nonlinear iteration is \(p+q+1\).
          In making sensible management of memory, only a constant \(q+2\) vectors must be stored in memory.
          To solve the least-squares problems defining the MPE and the RRE can be achieved by employing a QR decomposition, whose computational cost is \(\mathcal O(Nq^2)\) \citep{sidi_vector_2017}.

          % \paragraph{Connection to Krylov subspace methods}
          %
          % According to Sidi \citep{sidi_vector_2017}, the so-called Krylov subspace methods are closely related to the vector extrapolation methods presented above.
          % When the latter is applied to vector sequences obtained using fixed-point iterative methods to nonsingular linear systems of equations, they are mathematically equivalent.
          % More precisely, the MPE and the RRE methods are mathematically equivalent to the methods of Arnoldi and the GMRES.
          %
          % However, Krylov subspace methods and extrapolation methods differ in their algorithmic aspects entirely:
          % The only input of the former is a procedure that performs the matrix-vector multiplication without explicitly knowing the matrix coefficient matrix.
          % The latter takes as their only input a vector sequence that results from a fixed-point iterative scheme without knowing the matrix coefficient.
          %
          % In \cite{michler_interface_2005}, a Krylov-subspace method is proposed in the context of FSI.
          % However, as pointed out by Küttler and Wall \citep{kuttler_vector_2009}, the correct term for this approach should be a "Krylov-based vector extrapolation" method.
          % The method proposed can be obtained by applying the RRE to the sequence of residuals computed as
          % \(\Delta \mathbf{r}^*_i = \mathbf{x}^*_i - \mathbf{x}^k\), where the subscript \(i\) concerns the internal loop of the vector extrapolation method, and whose limit is \(\mathbf{0}\).
          % Küttler and Wall \cite{kuttler_vector_2009} argue that these residual differences have unfavorable numerical properties and should be avoided.

          % \subsection{Summary}
          % \label{sec:summary}
          %
          % Table~\ref{tab:mem_nr_func_eval_obs} provides information on the memory requirements up to a constant number of vectors, the number of function evaluations per nonlinear iteration, and some pertinent observations for each technique under consideration.
          %
          % \begin{landscape}
          %   \begin{table}[htbp]
          %     \caption{Comparison between several methods to the solution of nonlinear systems of equations. \(N\) denotes the number of unknowns, \(m\) denotes the number of previous iterations considered, \((p,q)\) is the ordered pair characterizing the vector extrapolation methods, and \(p\) is the size of the Krylov subspace.}
          %     \label{tab:mem_nr_func_eval_obs}
          %     % \setlength{\tabcolsep}{1pt}
          %     \centering
          %     \begin{tabular}{l cc p{10.5cm}}
          %       Method & \makecell[c]{Memory\\requirements} & \makecell[c]{No. function\\evaluations\\per iteration} & Observations\\
          %       \hline  \hline
          %       \multirow{3}{*}{\vphantom{\Big|}Fixed-point iteration} & \multirow{3}{*}{2 \(N\) vectors} & \multirow{3}{*}{1} &  Often diverges for strong coupling\\
          %              & & & Simplest method\\
          %              & & & Memory efficient\\
          %       \hline
          %       \multirow{3}{*}{Underrelaxation} & \multirow{3}{*}{2 \(N\) vectors} & \multirow{3}{*}{1} &  Simple\\
          %              & & & Improved stability over fixed-point\\
          %              & & & Need to choose a relaxation parameter manually\\
          %       \hline
          %       \multirow{3}{*}{Aitken relaxation} & \multirow{3}{*}{3 \(N\) vectors} & \multirow{3}{*}{1} &  Very popular in FSI\\
          %              & & &  Dynamic relaxation\\
          %              & & &  Improved stability over fixed-point\\
          %       \hline
          %       \multirow{2}{*}{\makecell[l]{Broyden-like family\\\citep{fang_two_2009}}} & \multirow{2}{*}{\makecell[c]{2 \(N\) vectors\\2 \((N\times (m-1))\) matrices}} & \multirow{2}{*}{1} &  \(\pazocal{O}(Nm^2)\) computation complexity (\(QR\) decomposition). \\
          %              & & &  Low number of function evaluations\\
          %       \hline
          %       \multirow{3}{*}{\makecell[l]{Broyden's method\\\citep{kelley_solving_2003}}} & \multirow{3}{*}{\makecell[c]{\((m+2)\) \(N\) vectors}} & \multirow{3}{*}{1} &  \(\pazocal{O}(N)\) computation complexity\\
          %              & & &  Low storage\\
          %              & & &  Superlinear convergence\\
          %       \hline
          %       \multirow{3}{*}{Newton-Krylov} & \multirow{3}{*}{\makecell[c]{Up to\\\((p+1)\) \(N\) vectors}} & \multirow{3}{*}{\(p+1^*\)} &  Large number of iterations possible\\
          %              & & &  Popular for the solution of systems of nonlinear equations\\
          %              & & &  Quadratic convergence under appropriate conditions\\
          %       \hline
          %       \multirow{2}{*}{\makecell[l]{Vector extrapolation\\ in cycling mode}} & \multirow{2}{*}{\((q+2)\) \(N\) vectors} & \multirow{2}{*}{\(n+q+1\)} &  Large number of function evaluations\\
          %              & & &  \(\pazocal{O}(Nq^2)\) computational complexity (\(QR\) decomposition)\\
          %       \hline\hline
          %       \multicolumn{4}{l}{\vphantom{\Huge |}\parbox{\textwidth}{\footnotesize{  \(^*\) The number of function evaluations in the Newton-Krylov methods depends on the number of iterations required for the convergence of the inner loop. There is a function evaluation per iteration of the inner loop.}}}
          %     \end{tabular}
          %   \end{table}
          % \end{landscape}


          % \subsection{Additional computational aspects}
          % \label{sec:addit-comp-aspects}

          % The following sections address some additional computational aspects that can complement the previous techniques.
          % In particular, the focus is placed on the prediction of the initial guess and the convergence criteria employed.

          % \subsubsection{Predictor} \label{sec:predictor}
          %
          % Iterative procedures are considered to solve the thermomechanical problem at a given time step \(n+1\).
          % As the first value approximating \(\mathbf{x}_{n+1}\), one can employ the converged value of the previous time step, \(\mathbf{x}_n\).
          % However, a very efficient way to increase the chances of stability and reduce computation time is to predict the optimal initial values at the beginning of every time step \citep{erbts_accelerated_2012, erbts_partitioned_2015, wendt_partitioned_2015}.
          % The prediction of the new solution by polynomial extrapolation is based on the converged solution of the last two or three time steps.
          % This method is based on polynomial vector extrapolation, which is relatively easy to implement, and the extra computational input is negligible.
          %
          % The maximum polynomial under consideration is of order two, i.e., the new solution is extrapolated from the results from the last three time steps.
          % The predictors $\mathbf{x}^{*}$ for the order $p=1$ and $p=2$ polynomials read
          % \begin{gather}
          %   p=1:\quad \mathbf{x}_{n+1}^{*}=2 \mathbf{x}_{n}-\mathbf{x}_{n-1}, \\
          %   p=2:\quad \mathbf{x}_{n+1}^{*}=3 \mathbf{x}_{n}-3 \mathbf{x}_{n-1}+\mathbf{x}_{n-2}.
          % \end{gather}
          % Slight adaptations must be considered if the time step is not constant throughout the simulation.

          % \subsubsection{Convergence criteria}
          %
          % For an iterative method to be useful, there must be reasonable criteria to determine its convergence.
          % The iteration residual is defined as
          % \begin{equation}
          %   \mathbf{r}^{k} = \bm{\mathcal{R}}(\mathbf{x}^{k}),
          % \end{equation}
          % and if it is equal to zero, then $\mathbf{x}$ is the solution to the system of nonlinear equations, hence, a reasonable convergence measure for the iteration procedure.
          %
          % The $l^{2}$-norm is used to obtain a scalar representative of the vectorial residual \(\mathbf{r}^{k}=\left(r^{k,1}, \ldots, r^{k,N}\right)^{T}\)
          % \begin{equation}
          %   \left\|\mathbf{r} ^{k}\right\|_{l^{2}}=\sqrt{\sum_{i}\left(r^{k, i}\right)^{2}},
          % \end{equation}
          % with the relative convergence criteria employed defined as
          % \begin{equation} \label{eq:def_res_used}
          %   \frac{\left\|\mathbf{r}^{k}\right\|_{l^{2}}}{\left\|\mathbf{x}^{k}\right\|_{l^{2}}}<\epsilon_\mathrm{rel} = \num{1e-10},
          % \end{equation}
          % setting the residual in relation to the current coupling iterate values.
